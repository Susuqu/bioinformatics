CNN basic knowledges
==========
- Record of some key notes by **QuSusu**.
- Some statements were directly copy from others.
- Start at 2018/04/23, last update at 2018/04/23.

----
**所谓卷积神经网络，就是会自动的对于一张图片学习出最好的卷积核以及这些卷积核的组合方式，也就是对于一张图片的任务来说，求出最好的图片对于本任务的特征的表达，然后来进行判断** 

[知乎：能否对卷积神经网络工作原理做一个直观的解释？](https://www.zhihu.com/question/39022858)

---

## I.基本概念（后续补充吧，今天来不及写这个了）
[机器学习相关的一个帖子](http://nooverfit.com/wp/category/cnn/)



## II. 参数设定 & 一个卷积过程的例子
### 1.参数设定
一般在我们进行一个CNN训练的时候，会涉及到两大类参数：超参数（Hyperparameters）和参数（parameters）。
- 超参数（Hyperparameters）：是我们根据经验人工给定的值。
- 参数（parameters）：是通过学习训练、不断改变得到的值，比如每次卷积核对应的权重，都是通过不断的给定一个值，然后根据学习情况调整直到找到最优。

超参数分别具体包括哪些过程的哪些值、如何给定这些值一个初始值、偏好值等等，详见下面的表格内容：

![超参数表格](%E8%B6%85%E5%8F%82%E6%95%B0%E8%A1%A8%E6%A0%BC.PNG)

---




|过程|超参数|如何给定对应的初始值|偏好值|举例|999|
|:---|:---|:---|:---|:---|---|
| 卷积 | 卷积层数 | 人工给定 | 好像没有偏好值 | 2||
|   | 卷积核大小 | 人工给定 | 奇数 | 3x3 ||
|   | 特定卷积层对应的输出数量 | 人工给定 | 无 | 10 ||
| 池化 | 池化层数 | 人工给定 | 无 | 2 ||
|   | 池化后的输出大小 | 人工给定 | 一般会缩小 | 2x2 ||
| Relu | Relu的次数 | 人工给定 | 无 | 2 ||
| 全连接 | 全连接的次数 | 人工给定 | 无 | 2 ||
|   | 全连接的输出特征值个数 | 人工给定 | 无 |首次180，第二次为3 ||
| 其他 | 步长（stride） | 人工给定 | 无 | 2 ||
|   | 边界（padding） | 人工给定 | 无 | zero padding ||








---

### 2.一个卷积过程的例子
在实际做卷积的过程，可能最困惑的就是卷积核大小的选取、数据是由几维变成了几维、CNN的过程是如何的等等问题，那么结合上面关于参数的说明，来看一个非常简单的例子帮助理解数据的维度变换等细节内容：这是一个比较简单的例子：对每一副图片用两层卷积、两层池化、两次relu、两次全连接进行学习，对最后一次全连接的结果（n个特征）通过softmat函数进行分类。
![一个卷积过程说明的例子](%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B%E8%AF%B4%E6%98%8E%E7%9A%84%E4%BE%8B%E5%AD%90.png)

### 3.其他
- 池化的作用：降维，或者说是为了更好的放大特征值之间的差别。
- 全连接：整合所有的特征值然后根据我们给定的个数提取其中关键的特征值，其实也是降维的一种方式（全连接的输出特征值的个数，也是我们人工给定的）。
- relu函数：非线性的转换，其实就是一个分类函数，会把矩阵中所有负数的值变为0。 
- softmax函数：根据经过最后一次全连接的特征值进行分类。softmax函数的作用呢，一是分类，二是给出量化的指标来说明有多大的比例可以分为该类别。





